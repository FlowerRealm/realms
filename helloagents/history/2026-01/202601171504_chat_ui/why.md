# 变更提案: 用户对话功能（按分组固定渠道）

## 需求背景

Realms 当前已提供 OpenAI 兼容的数据面接口（`/v1/responses`、`/v1/chat/completions`、`/v1/models`）与 Web 控制台，但普通用户若要“直接对话/试用模型”，仍需要：
- 手动创建 Token
- 自己写/找客户端（Codex CLI、SDK、curl 等）

这会增加使用门槛与支持成本。

本需求希望新增一个面向普通用户开放的“对话页面”，用户登录后即可在浏览器中直接发起对话请求；请求通过本地 `POST /v1/responses` 发出（沿用计费/订阅/用量逻辑），并自动创建一个名为 `chat` 的数据面 API Key 供页面调用。

同时，需要“对话分组 → 渠道”的单独配置入口：管理员在后台为每个分组指定对话流量固定走哪条上游渠道，确保对话流量可控、可治理。

## 产品分析

### 目标用户与场景

- **普通用户**
  - 登录 Web 控制台后，直接在浏览器中与模型对话
  - 不需要理解 Token、Base URL、wire api 等概念
- **管理员（root）**
  - 按分组治理对话流量：为不同分组选择不同的上游渠道（成本/稳定性/额度策略）
  - 对话能力需要与现有计费/配额体系一致（不引入第二套扣费口径）

### 价值主张与成功指标

- **价值主张**
  - 降低使用门槛：无需额外客户端即可体验模型
  - 降低支持成本：减少“如何配置 Token/CLI”的重复咨询
  - 增强治理能力：对话流量可按分组集中到指定渠道
- **成功指标（可验收）**
  - 用户首次进入对话页即可发起对话（自动创建 `chat` Token）
  - 对话请求可正常计费并出现在用量统计中（沿用 `/v1/responses`）
  - 对话历史在浏览器 `localStorage` 可恢复（服务端不持久化对话内容）
  - 管理员配置“对话分组 → 渠道”后，用户对话请求强制走该渠道

### 人文关怀

- **隐私提示与可控性**
  - 对话内容保存在浏览器本地 `localStorage`，页面应明确提示“清理浏览器存储会丢失对话记录”
  - 提供“一键清空本地对话记录”能力
- **最小化数据保留**
  - 服务端不新增对话内容持久化（不引入对话消息表），避免扩大敏感数据存储面

## 变更内容

1. 新增用户侧对话页面（`/chat`）：支持输入消息、选择模型、流式展示输出；对话记录仅保存到浏览器 `localStorage`。
2. 用户侧自动创建/获取一个名为 `chat` 的数据面 Token（`rlm_...`），页面使用该 Token 调用本地 `POST /v1/responses`。
3. 新增“对话分组路由”配置：按 `group_name` 绑定固定 `upstream_channel_id`，所有来自对话页面的请求强制走该渠道。
4. 管理后台新增页面用于配置“对话分组 → 渠道”（含校验：渠道必须启用且包含该分组）。
5. 计费/订阅/用量统计沿用现有逻辑（不新增第二套扣费流程）。

## 影响范围

- **模块:**
  - `internal/web/*`（对话页、token 自助、模型列表）
  - `internal/admin/*`（对话分组路由管理页）
  - `internal/api/openai/*`（对话请求路由约束）
  - `internal/store/*` + `internal/store/migrations/*`（对话分组路由表与读写）
- **API:**
  - 用户侧：`GET /chat`、`POST /api/chat/token`（用于首次发放/重置 token）、`GET /api/chat/models`（按对话渠道过滤可用模型）
  - 管理侧：`GET /admin/chat-routes`、`POST /admin/chat-routes`、`POST /admin/chat-routes/{group_name}/delete`（或等价路由）
  - 数据面：`POST /v1/responses`（保持不变，仅在“对话标识”存在时新增路由约束）
- **数据:**
  - 新增表：`chat_group_routes`（`group_name` → `upstream_channel_id`）

## 核心场景

### 需求: 用户可在 Web 前端对话
**模块:** web
用户登录后打开对话页面即可直接对话，不需要额外配置。

#### 场景: 登录后开始一次流式对话
用户已登录，打开 `/chat`，选择一个模型并发送消息。
- 页面以 SSE 方式展示增量输出
- 对话请求通过本地 `POST /v1/responses` 发出并成功返回

### 需求: 自动创建 `chat` Token 并在浏览器使用
**模块:** web/store
对话页面需要一个数据面 Token 调用 `/v1/responses`，用户不应手工创建。

#### 场景: 首次进入对话页自动可用
用户首次打开 `/chat`（浏览器尚无 token）。
- 服务端为该用户创建名为 `chat` 的 Token，并一次性下发给页面写入 `localStorage`
- 后续请求使用该 token 鉴权，沿用现有订阅/用量体系

#### 场景: token 丢失或失效可自助恢复
用户清空浏览器存储或 token 被撤销导致 401。
- 页面可触发“重置 token”，服务端轮换生成新的 `chat` token 并回写 `localStorage`
- 旧 token 被撤销，避免残留可用密钥

### 需求: 管理员配置对话分组路由
**模块:** admin/store
管理员需要为每个分组指定对话流量走哪条渠道。

#### 场景: root 为某分组绑定渠道
root 打开管理后台对话分组页面，为 `vip` 分组选择渠道 `OpenAI 官方` 并保存。
- 保存时校验渠道启用且 `upstream_channels.groups` 包含 `vip`
- 保存后立刻生效

### 需求: 对话请求强制走分组绑定渠道
**模块:** openai handler
对话页面发起的请求必须固定走该分组配置的渠道，避免被调度到其他渠道。

#### 场景: 对话请求命中固定渠道
用户通过对话页发起 `POST /v1/responses`，请求携带对话标识（例如 `X-Realms-Chat: 1`）。
- 服务端根据用户所属分组计算对话分组，并查 `chat_group_routes` 得到固定 `channel_id`
- 调度器在该 `channel_id` 内选择 credential（允许 key 级 failover），但禁止跨 channel failover
- 若该渠道未绑定用户所选模型，返回明确错误提示

## 风险评估

- **风险:** 浏览器 `localStorage` 保存 token 与对话记录，存在 XSS 盗取风险  
  **缓解:** SSR 页面严格转义用户输入；不在 URL/日志中输出 token；token 专用于对话且支持一键轮换；默认不把对话内容持久化到服务端。
- **风险:** 管理员配置错误（渠道未绑定某模型）导致用户对话失败  
  **缓解:** 对话页模型下拉仅展示“对话渠道可用模型”；服务端错误信息明确指向“渠道未绑定模型”。
- **风险:** 每次对话请求多一次 DB 查询（路由表）  
  **缓解:** 查询仅在“对话标识”请求中发生；可按需要引入短 TTL 内存缓存（非本期强制）。

